# -*- coding: utf-8 -*-
"""hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JS0MsQPK3WkV3Zy6M9qjGihuWR-GhEFY

# Homework 6: Coding

**Due Tuesday October 26th, 11:59pm.**

**This assignment can be done individually or in groups of two.**

**To submit the coding portion of hw6, provide a link to your Colab notebook on Gradescope (One per group and don't forget to add your partner's name). Be sure to modify the restriction before sharing.**
"""

"""
Import required libraries.
You should not import any other packages without explicit permission.
"""

import numpy as np
np.set_printoptions(precision=4)
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine, fetch_olivetti_faces
from sklearn.preprocessing import StandardScaler

"""# Question 2: Simple PCA

In this assignment, we will implement Principal Component Analysis and perform it on a simple 2-dimensional dataset. One way to perform PCA is by doing an eigenvalue decomposition of a data covariance matrix, which we will implement here.
"""

"""
Load data (Two features from Sk-Learn Wine Dataset).
DO NOT MODIFY.
"""

data = load_wine().data[:, [0, 12]]
X = StandardScaler().fit_transform(data)

"""## PCA Implementation

First fill out these helper functions. **Please do NOT use np.cov in this section. You may find np.linalg.eig useful.**
"""

def get_cov_mat(X):
    """
    Returns a covariance matrix of the given input matrix.
    You may asssume the mean of X on each feature centers at 0.
    
    Args:
        X ((n,p) np.ndarray): The input data
    Returns:
        X_cov ((p,p) np.ndarray): The covariance matrix of the input data
    """  
    
    n, p = X.shape
    
    # <---- Start your code here ----->
    X_cov = (X.T @ X) / (n-1)
    # <---- End your code here ----->
    
    # confirm the shape of your output
    assert X_cov.shape == (p,p)

    # confirm the result of your output
    assert np.allclose(X_cov, np.cov(X.T))
    
    return X_cov



def get_eig(cov_mat):
    """
    Returns eigenvectors and eigenvalues in sorted order by eigenvalue
    given input covariance matrix.
    
    Args:
        cov_mat ((p,p) np.ndarray): The input covariance matrix
    Returns:
        eig_vals_sorted ((p,) np.ndarray): Eigenvalues in decreasing order
        eig_vecs_sorted ((p,p) np.ndarray): Eigenvectors corresponding to the 
                                            sorted eigenvalues, eig_vecs_sorted[:, i]
                                            corresponds to the ith vector
                                            (i.e. each column is an eigenvector)
    """
    
    p = cov_mat.shape[1]

    # <---- Start your code here ----->
    w, v = np.linalg.eig(cov_mat)

    idx = np.argsort(-w)
    eig_vals_sorted = w[idx]
    eig_vecs_sorted = v[:,idx]
    # <---- End your code here ----->  
    
    # confirm the shapes of your outputs
    assert eig_vals_sorted.shape == (p,)  
    assert eig_vecs_sorted.shape == (p,p)  
        
    return eig_vals_sorted, eig_vecs_sorted
  
    

def get_projection(X, principal_components):
    """
    Returns the projection of X onto the principal components.
    
    Args:
        X ((n,p) np.ndarray): The input data
        principal_components ((p,p) np.ndarray): Principal components of data
    Returns:
        projection ((n,p) np.ndarray): The projection of X onto the components
    """  

    n, p = X.shape
    
    # <---- Start code here ----->
    projection = X @ principal_components
    # <---- End code here ----->  

    # confirm the shape of your output
    assert projection.shape == (n,p)

    return projection

"""Next, fill out this main function for PCA using the helper functions above"""

def pca(X):
    """
    Performs PCA on dataset X. 
    
    Args:
        X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) 
                              by p (number of features) matrix                 
    
    Returns:
        projection ((n, p) np.ndarray): Samples projected on reduced dimensions
        values ((p,) np.ndarray): Singular Values
        components ((p, p) np.ndarray): Principal Components (also eigenvectors)
    """

    n, p = X.shape

    # <---- Start code here ----->
    cov_mat = get_cov_mat(X)
    eigen_val, eigen_vec = get_eig(cov_mat)

    projection = get_projection(X, eigen_vec)
    values = eigen_val
    components = eigen_vec
    # <---- End code here ----->

    # confirm the shapes of your outputs
    assert projection.shape == (n,p)
    assert values.shape == (p,)
    assert components.shape == (p,p)

    return projection, components, values

"""### 2.1 Answer on LaTeX

Call your pca function on sample 2-dimensional set, centering X on its mean first. Report the eigenvectors and eigenvalues. 
"""

X_mean = np.mean(X, axis=0)
projected, components, values = pca(X - X_mean)
print(components)
print(values)

"""### 2.2 Answer on LaTeX

Plot the given points (with both axes in the same scale) as well as the arrows representing the principal components in original space, with x1 in the x-axis and x2 in the y-axis. The principal component arrows should originate at the mean and have magnitudes equal to their corresponding eigenvalues.
"""

plt.axis('equal')
plt.arrow(X_mean[0], X_mean[1], components[0, 0]*values[0], components[1, 0]*values[0], ec='red', head_width=0.1)
plt.arrow(X_mean[0], X_mean[1], components[0, 1]*values[1], components[1, 1]*values[1], ec='green', head_width=0.1)
plt.scatter(X[:, 0], X[:, 1], s=10)
plt.show()

"""### 2.3 Answer on LaTeX

Now plot the given points (with both axes in the same scale) in principal component space, with x-axis representing the projection of the first component and the y-axis representing the component on the second. 
"""

plt.axis('equal')
plt.scatter(projected[:, 0], projected[:, 1], s=10)
plt.show()

"""# Question 3: Eigenfaces

Now we will perform PCA on images of faces and see how reducing the dimensions of our images affects the images themselves.
"""

"""
Load data (Faces Dataset).
DO NOT MODIFY.
"""

faces = fetch_olivetti_faces(shuffle=True, random_state=10)

"""
Preprocess data.
"""

n_samples, h, w = faces.images.shape
X_images = faces.data
y_names = faces.target
n_features = X_images.shape[1]

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)

"""
Help visualising the portraits from the dataset.
"""

def plot_portraits(images, titles, h, w, n_row, n_col):
    """
    Plots portraits of images
    
    Args:
        images ((n,p) np.ndarray): The input images in a vector format
        titles (list string): The list of names for each image
        h (int): The input height of the portraits
        w (int): The input width of the portraits
        n_row (int): The number of rows to display of images
        n_col (int): The number of columns to display of images
    """  
    
    plt.figure(figsize=(2.2 * n_col, 2.2 * n_row))
    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.20)
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
        plt.title(titles[i])
        plt.xticks(())
        plt.yticks(())

plot_portraits(X_images, y_names, h, w, n_row=4, n_col=8)

"""## PCA with SVD
PCA can also be implemented using Singular Value Decomposition on the data itself. Implement an alternative form of PCA below using SVD **Please call the np.linalg.svd function for SVD**.
"""

def pca2(X, n_pc):
    """
    Performs PCA on dataset X
    
    Args:
        X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) 
                              by p (number of features) matrix                 
        n_pc: The number of principal components
    
    Returns:
        projected ((n, min(n, n_pc)) np.ndarray): Samples projected on reduced dimensions
        components ((min(n, n_pc), p) np.ndarray): Principal Components (also eigenvectors)
        values ((min(n, n_pc),) np.ndarray): Singular Values    
        mean ((p,) np.ndarray): Mean of features
        centered_data ((n, p) np.ndarray): Data with the mean subtracted
    """
    
    n, p = X.shape

    # <---- Start code here ----->
    avg = np.mean(X, axis = 0)
    Xc = X - avg

    u, s, vh = np.linalg.svd(Xc)

    if n < n_pc:
      k = n
    else:
      k = n_pc

    s = s[:k]
    vh = vh[:k, :]

    projected = np.dot(Xc, vh.T)
    components = vh
    values = s
    mean = avg
    centered_data = Xc
    # <---- End code here -----> 
    
    assert projected.shape == (n, min(n, n_pc))
    assert components.shape == (min(n, n_pc), p)
    assert values.shape == (min(n, n_pc),)
    assert mean.shape == (p,)
    assert centered_data.shape == (n, p)

    return projected, components, values, mean, centered_data

"""Now run the script below that takes the image matrix of n rows by h*w features and runs PCA on this X with 50 components and plot the images of the eigenvectors (which you will have to unwrap back into the image shape)."""

n_components = 50
P, C, V, M, Y = pca2(X_images, n_components)

eigenfaces = C.reshape((n_components, h, w))
eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_portraits(eigenfaces, eigenface_titles, h, w, 4, 4)

# To report your singular values
print(V[:5])

"""### 3.1 Answer on Latex

Report the first five singular values. Paste the eigenfaces on LaTeX.

## Reconstructing faces
Now implement a function to reconstruct a face given projected data points
"""

def reconstruction(P, C, M, h, w, image_index):
    """
    Reconstructs an image with the given components
    
    Args:
        P, projected ((n, n_pc) np.ndarray): Samples projected on reduced dimensions
        C, components ((n_pc, p) np.ndarray): Principal Components (also eigenvectors)
        M, mean ((p,) np.ndarray): Mean of features
        h (int): The input height of the portraits
        w (int): The input width of the portraits
        image_index (int):  The index of the image in the projected data matrix
        
    Returns:
        recovered_image ((h, w) or (p,) np.ndarray): Reconstructed image from the inputs
    """    
    
    # <---- Start code here -----> 
    recovered_image = np.dot(P[image_index,:], C) + M
    # <---- End code here -----> 
    
    return recovered_image

"""Now run the script below to see reconstrcted faces."""

recovered_images = [reconstruction(P, C, M, h, w, i) for i in range(len(X_images))]
plot_portraits(recovered_images, y_names, h, w, n_row=4, n_col=8)

"""### 3.2 Answer on Latex
Paste the portrait reconstructions on LaTeX.

## Explanation of Variance and Reconstruction Error

Make 3 plots. 
1. One with the reconstruction error over the number of components used. Plot the average of the reconstruction error for the first 50 images over reconstructions with varying number of components, normalized to the range of 0-1.
1. One with the eigenvalues from largest to smallest. 
1. And lastly, one with the cumulative (sum of) eigenvectors used per number of principal components (this is known as explained variance), normalized to the range of 0-1.
"""

P, C, eig_vals, M, Y = pca2(X_images, n_features)

reconst_errors = []
for i in range(X_images.shape[0]):
    dist = 0
    for j in range(50):    
        reconst_img = reconstruction(P[:, :i+1], C[:i+1, :], M, h, w, j)
        dist += np.linalg.norm(np.array(reconst_img).flatten() - np.array(X_images[j]))
    reconst_errors.append(dist/20)

"""
Plot reconstruction error for one image normalized.
"""

plt.plot(reconst_errors/reconst_errors[0], label="reconstruction error")
plt.legend()
plt.show()

"""
Plot eigenvalues.
"""

plt.plot(eig_vals, label="eigenvalues")
plt.legend()
plt.show()

"""
Plot cumulative eigenvalues over the number of components used.
"""

tot = sum(eig_vals)
var_exp = [(i / tot) for i in sorted(eig_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp)

plt.plot(cum_var_exp, label="cumulative eigenvalues")
plt.legend()
plt.show()

"""### 3.3 Answer on Latex

Paste the graphs into LaTeX.

# Turning it in

To submit the coding portion of hw6, provide a link to your Colab notebook on Gradescope (One per group and don't forget to add your partner's name). Be sure to modify the restriction before sharing.
"""