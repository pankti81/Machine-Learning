# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1esoAmda3q-f-hVjtV1V3yMbTKiBeWpOK

# Homework 2: Coding

**Due Sunday September 26th, 11:59pm.**

**This assignment can be done individually or in groups of two.**

**To submit the coding portion of hw2, provide a link to your Colab notebook on Gradescope (One per group and don't forget to add your partner's name). Be sure to modify the restriction before sharing.**

**Feel free to modify anything in this notebook unless notified.**

### Imports and Data
"""

"""
Import required libraries.
You should not import any other packages without explicit permission.
"""
import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt

"""
Run the following code to upload and unzip the data into the Colab environment.
"""
from google.colab import files
uploaded = files.upload()
!unzip hw2_data.zip

"""## Q3: Least Squares Regression (40 points)

Implement the following functions for question 3. Please **do not** use the sklearn implementation of linear regression or other imports beyond those listed above.

### Q3.1: Implementing Linear Regression (11 points)

#### L1 and L2 error (2 points + 2 points)
"""

def L1_error(y, y_hat):
    """
    L1 error loss
    
    Args:
    y ((n,1) np.array): actual labels
    y_hat ((n,1) np.array): estimated labels
    
    Returns:
        float: L1 error
    """
    # <---- Your code starts here ---->
    L1_error_ = np.sum(np.abs(y-y_hat))/y.shape[0]
    # <---- Your code ends here ---->
    
    return L1_error_
    
    
    
def L2_error(y, y_hat):
    """
    L2 error loss
    
    Args:
    y ((n,1) np.array): actual labels
    y_hat ((n,1) np.array): estimated labels
    
    Returns:
        float: L2 error
    """
    # <---- Your code starts here ---->
    L2_error_ = np.sum(np.square(y-y_hat))/y.shape[0]
    # <---- Your code ends here ---->

    return L2_error_

"""#### Least Square Regression (3 points)"""

def LinearRegression(train_data, train_labels):
    """
    Runs OLS on the given data.
    
    Args:
        train_data ((n,p) np.array): n is the number of training points and p the number of features
        train_labels ((n,1) np.array): training labels for the training data    
    
    Returns
        tuple: (w, b) where w is a (p,1) weight vector, and b the bias term     
    """
    # <---- Your code starts here ---->
    # col = np.full((train_data.shape[0],1),1)
    # train_data = np.hstack((col,train_data))
    # print(train_data)

    w = np.matmul(np.linalg.inv(np.matmul(np.transpose(train_data),train_data)),(np.matmul(np.transpose(train_data),train_labels)))
    # print(w_full.shape)
    # print(w_full[:,1:].shape)
    # w, b = w_full[1:,:], w_full[0,:]
    # <---- Your code ends here ---->
    
    return w

LinearRegression(np.transpose(np.array([[2, 1, 1]])), np.transpose(np.array([[5, 8, 2]])))

"""#### Ridge Regression (4 points)"""

def LinearRegressionL2(train_data, train_labels, lambda_):
    """
    Runs linear regression with L2 regularization (ridge) on the given data.
    
    Args:
        train_data ((n,p) np.array): n is the number of training points and p the number of features
        train_labels ((n,1) np.array): training labels for the training data    
        lambda_ (float): scalar weighting the L2 penalty

    Returns
        tuple: (w, b) where w is a (p,1) weight vector, and b the bias term  
    """
    # <---- Your code starts here ---->
    col = np.full((train_data.shape[0],1),1)
    train_data = np.hstack((col,train_data))

    w_full = np.matmul(np.linalg.inv(np.matmul(np.transpose(train_data),train_data)+(lambda_*np.eye(train_data.shape[1]))),(np.matmul(np.transpose(train_data),train_labels)))
  
    w, b = w_full[1:,:], w_full[0,:]
    # <---- Your code ends here ---->
    
    return (w, b)

"""### Q3.2: Data Set 1 (synthetic 1-dimensional data) (8 points)"""

X_train = pd.read_csv('/content/hw2_data/Data_set_1/x_train.txt', header=None).values
y_train = pd.read_csv('/content/hw2_data/Data_set_1/y_train.txt', header=None).values
X_test = pd.read_csv('/content/hw2_data/Data_set_1/x_test.txt', header=None).values
y_test = pd.read_csv('/content/hw2_data/Data_set_1/y_test.txt', header=None).values

"""#### Q3.2.1: Learning Curve (4 points)

Use your implementation of unregularized least squares regression to learn a regression model from first 10$\%$ of the training data, then 20$\%$ of the training data, then 30$\%$ and so on up to 100$\%$. In each case, measure both the $L_1$ and $L_2$ error on the training examples used, as well as the error on the given test set. Plot a curve showing both errors (on the *y-axis*) as a function of the number of training examples used (on the *x-axis*).

Add the resulting curve to your Latex document.
"""

L1_train_errors = [0] * 10
L2_train_errors = [0] * 10
L1_test_errors = [0] * 10
L2_test_errors = [0] * 10

# <---- Your code starts here ---->
for i in range(10, 110, 10):
  w_l, b_l = LinearRegression(X_train[0:i,:],y_train[0:i,:])

  y_hat_train = b_l + np.matmul(X_train[0:i,:], w_l)
  y_hat_test = b_l + np.matmul(X_test, w_l)

  L1_train_errors[int((i/10)-1)]=(L1_error(y_train[0:i,:], y_hat_train))
  L1_test_errors[int((i/10)-1)]=(L1_error(y_test, y_hat_test))
  L2_train_errors[int((i/10)-1)]=(L2_error(y_train[0:i,:], y_hat_train))
  L2_test_errors[int((i/10)-1)]=(L2_error(y_test, y_hat_test))
# <---- Your code ends here ---->

plt.plot(range(10), L1_train_errors, label ='L1_train')
plt.plot(range(10), L1_test_errors, label ='L1_test')
plt.plot(range(10), L2_train_errors, label = 'L2_train')
plt.plot(range(10), L2_test_errors, label = 'L2_test')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
plt.title("Learning Curve")
plt.xlabel("Percent of Training Data")
plt.xticks(range(10), range(10,101,10))
plt.ylabel("Error")
plt.show()

"""#### Q3.2.2: Analysis of model learned from full training data (4 points)

Write down the weight and bias terms, $\hat{w}$ and $\hat{b}$, learned from the full training data in your Latex document. Also, write down the $L_2$ training and test error of this model. In a single figure, draw a plot of the learned linear function (input instance on the *x-axis* and the predicted value on the *y-axis*), along with a scatter plot depicting the true label associated with each test instance.

Add the resulting plot to your Latex document.
"""

# <---- Your code starts here ---->
w_l, b_l = LinearRegression(X_train,y_train)
y_hat_train = b_l + np.matmul(X_train, w_l)
y_hat_test = b_l + np.matmul(X_test, w_l)

L2_train_errors = (L2_error(y_train, y_hat_train))
L2_test_errors = (L2_error(y_test, y_hat_test))
print(w_l,b_l)
print(L2_train_errors, L2_test_errors)
# <---- Your code ends here ---->

x = np.linspace(-1, 1)
plt.plot(x,(x * w_l + b_l).T, color='red')
plt.scatter(X_test, y_test, s=10)
plt.title('Model from Full Training Data')
plt.show()

"""### Q3.3: Data Set 2 (real 12-dimensional data) (21 points)

#### Q3.3.1-3.3.2: Regression on different portions of training data (6 points + 6 points)

#### Regression on 5$\%$ of the training data. 
Use your implementation of $L_2$-regularized least squares regression to learn a model on 5$\%$ of the training data. Select the regularization parameter from the range $\{$0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10,20,50,100,200,500,1000,2000,5000$\}$ using 5-fold cross validation on the relevant training data. Draw a plot showing $\lambda$ on the *x-axis* and the training, test, and cross validation errors on the *y-axis* using the $L_2$ error. 

Then record the chosen value of $\lambda$ along with the weight vector, bias term, and all corresponding errors for the chosen value of $\lambda$.
        
#### Regression on 100$\%$ of the training data.

Repeat the above process, but instead learn from the full training data for $L_2$-regularized regression. Plot all of the errors, and record the chosen value of $\lambda$ along with the weight vector, bias term, and all corresponding errors for the chosen value of $\lambda$.

Add the resulting curves to your Latex document.
"""

for percent in [5, 100]:
    lambdas = [0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10,20,50,100,200,500,1000,2000,5000]
    
    X_test = pd.read_csv('/content/hw2_data/Data_set_2/X_test.txt', header=None).values
    y_test = pd.read_csv('/content/hw2_data/Data_set_2/y_test.txt', header=None).values
    X_train = pd.read_csv('/content/hw2_data/Data_set_2/X_train_'+str(percent)+'.txt', header=None).values
    y_train = pd.read_csv('/content/hw2_data/Data_set_2/y_train_'+str(percent)+'.txt', header=None).values

    # <---- Your code starts here ---->
    train_error = []
    cv_error = []
    test_error = []
    weights_list=[]
    bias_list=[]

    a = int(X_train.shape[0]/5)
    for lamda in lambdas:
      k = 5
      error=0
      for folds in range(k):
        # in_train = np.concatenate((X_train[],X_train))
        in_train = np.concatenate((X_train[0:a*folds,:], X_train[a*(folds+1):,:]))
        out_train = np.concatenate((y_train[0:a*folds,:], y_train[a*(folds+1):,:]))

        in_test = X_train[(a*folds):(a*(folds+1))]
        out_test = y_train[(a*folds):(a*(folds+1))]

        w, b = LinearRegressionL2(in_train, out_train, lamda)
        y_hat_train = b + np.matmul(in_train, w)
        y_hat_test = b + np.matmul(in_test, w)

        error += L2_error(out_test, y_hat_test)

      cv_error.append(error/k)

      w, b = LinearRegressionL2(X_train, y_train, lamda)
      weights_list.append(w)
      bias_list.append(b)

      y_hat_train = b + np.matmul(X_train, w)
      y_hat_test = b + np.matmul(X_test, w)

      train_error.append(L2_error(y_train, y_hat_train))
      test_error.append(L2_error(y_test, y_hat_test))

    #Optimum lambda
    index = np.argsort(cv_error)[0]

    print(percent, 'regression')
    print('----------------')
    print('Optimal Lambda: ', lambdas[index])
    print('Train Error: ', train_error[index])
    print('Test Error: ', test_error[index])
    print('Cross Validation Error: ', cv_error[index])
    print('Weight Vector: ', weights_list[index])
    print('Bias: ', bias_list[index])



    # <---- Your code ends here ---->
        
    plt.plot(range(len(lambdas)), train_error, label='training error')
    plt.plot(range(len(lambdas)), cv_error, label='cv error')
    plt.plot(range(len(lambdas)), test_error, label='test error')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
    plt.title('Regression on ' + str(percent) + '% of the training data')
    plt.ylabel('L2 error')
    plt.xlabel('Lambda Value')
    plt.xticks(range(len(lambdas)), lambdas, rotation=45)
    plt.show()

"""#### Q3.3.3-3.3.4: Report on Latex (5 points + 4 points)
Answer the following questions on Latex in the respective section.

3.2.3 For each of the two training sets considered above (5$\%$ and 100$\%$), compare the training and test errors of the models learned using unregularized least squares regression and ridge regression. What can you conclude from this about the value of regularization for small and large training sets?

3.2.4 For each of the two training sets considered above (5$\%$ and 100$\%$), Which $\lambda$ should be larger by theory?why? Do those values align with the conclusion you made in part 3.2.3?

"""

for percent in [5, 100]:
    
    X_test = pd.read_csv('/content/hw2_data/Data_set_2/X_test.txt', header=None).values
    y_test = pd.read_csv('/content/hw2_data/Data_set_2/y_test.txt', header=None).values
    X_train = pd.read_csv('/content/hw2_data/Data_set_2/X_train_'+str(percent)+'.txt', header=None).values
    y_train = pd.read_csv('/content/hw2_data/Data_set_2/y_train_'+str(percent)+'.txt', header=None).values

    w, b = LinearRegression(X_train,y_train)
    y_hat_train = b + np.matmul(X_train, w)
    y_hat_test = b + np.matmul(X_test, w)

    L2_train_errors = (L2_error(y_train, y_hat_train))
    L2_test_errors = (L2_error(y_test, y_hat_test))

    print(percent,'Regression')
    print('L2 Train Error: ', L2_train_errors)
    print('L2 Test Error: ', L2_test_errors)

"""## Turning it in

To submit the coding portion of hw2, provide a link to your Colab notebook on Gradescope (One per group and don't forget to add your partner's name). Be sure to modify the restriction before sharing.
"""